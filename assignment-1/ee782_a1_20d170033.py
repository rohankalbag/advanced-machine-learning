# -*- coding: utf-8 -*-
"""EE782-A1-20d170033.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G79-T95rNEgkOj2TpBfuNli1zRGyNsHB

# Using LSTMs for Stock Market Prediction

## EE 782 Assignment 1
## Name : Rohan Rajesh Kalbag
## Roll: 20D170033

## Using Kaggle API for frequent and fast download of Dataset, unable to use mounting GDrive after restrictions added by CC on Google Services
"""

!pip install -q kaggle
!mkdir ~/.kaggle
!wget https://rohankalbag.github.io/kaggle.json
!mv ./kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets list
!kaggle datasets download -d rohanrkalbag/ee782

# include libraries
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import numpy as np
import torch

"""### Chosen Stocks"""

# choose the stocks which want to be included for processing later
chosen_stocks = ['AMD', 'ADBE', 'AAPL', 'AMZN']

import zipfile

zip_file_path = './ee782.zip'

output_directory = './'

# Open the .zip file and selectively extract the files corresponding to these stocks
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    for file_name in chosen_stocks:
        try:
            zip_ref.extract('sp500_tickers_A-D_1min_1pppix/%s_1min.txt' % file_name, output_directory)
            print(f'Extracted {file_name} to {output_directory}')
        except KeyError:
            print(f'File {file_name} not found in the archive.')

print('Unzip process completed.')

"""## 1a) Minute by Minute Closing Price for a Few Stocks"""

def get_subset_of_dataset(stockname, start_date, end_date):
    # function which returns a subset of data, given start and end date, and stock name
    filename = 'sp500_tickers_A-D_1min_1pppix/%s_1min.txt' % stockname
    df = pd.read_csv(filename, header=0, names=["datetime", "open", "high", "low", "close", "volume"]) # add column names
    df['datetime'] = pd.to_datetime(df['datetime'], format='%Y-%m-%d %H:%M:%S') # convert to datetime
    start_date = pd.to_datetime(start_date)
    end_date = pd.to_datetime(end_date)
    df = df[(df['datetime'] < end_date) & (df['datetime'] >= start_date)] # obtain subset from start date to end date
    return df

def visualize_stock_prices_min(df, title):
    fig, axes = plt.subplots(1, 2, figsize=(13, 6), sharex=True)
    column_names=['open', 'close', 'high', 'low']

    for i, col_name in enumerate(column_names):
      axes[0].plot(df['datetime'], df[col_name], '-s', label=col_name)

    # visualize minute wise stocks prices in one subplot
    axes[0].set_title("Stock Prices")
    dtFmt = mdates.DateFormatter('%Y-%m-%d %H:%M')
    axes[0].xaxis.set_major_formatter(dtFmt)
    axes[0].xaxis.set_tick_params(rotation=90, labelsize='x-small')
    axes[0].legend(loc='upper left')

    # visualize minute wise volumes in the other subplot
    axes[1].plot(df['datetime'], df['volume'], '-s', label='volume')
    axes[1].set_title("Volume of Stock Traded")
    dtFmt = mdates.DateFormatter('%Y-%m-%d %H:%M')
    axes[1].xaxis.set_major_formatter(dtFmt)
    axes[1].xaxis.set_tick_params(rotation=90, labelsize='x-small')
    axes[1].legend(loc='upper left')

    plt.xlabel("timestamp")
    plt.suptitle(title, fontsize=13)
    plt.tight_layout(rect=[0, 0, 1, 0.97])  # Adjust subplot layout
    plt.show()

"""### Let us print for the case of January 3rd 2020 from 12:00 pm to 12:30 pm for each of the stocks"""

for i in chosen_stocks:
  df = get_subset_of_dataset(i, start_date='2020-01-03 12:00:00', end_date='2020-01-03 12:30:00')
  visualize_stock_prices_min(df, f"Minute by Minute Stock Prices for {i} on 3rd Jan 2020 from 12 pm to 12:30pm")

"""## 1b) Day by Day Closing Price for Few Stocks"""

def visualize_stock_prices_day(df, title):
    # index dataframe using the datetime column
    df.set_index('datetime', inplace=True)
    # resample the data day-wise using time index, perform aggregations according to the column
    # for opening take the start price, for high take the maximum price, for low take minimum, close take the last one, for volume take the sum
    df = df.resample('D').agg({'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last', "volume" : "sum"})

    fig, axes = plt.subplots(2, 1, figsize=(13, 10), sharex=True)
    column_names=['open', 'close', 'high', 'low']

    for i, col_name in enumerate(column_names):
      axes[0].plot(df.index, df[col_name], label=col_name)

    axes[0].set_title("Stock Prices")
    dtFmt = mdates.DateFormatter('%Y-%m-%d %H:%M')
    axes[0].xaxis.set_major_formatter(dtFmt)
    axes[0].xaxis.set_tick_params(rotation=90, labelsize='x-small')
    axes[0].legend(loc='upper left')

    axes[1].plot(df.index, df['volume'], label='volume')
    axes[1].set_title("Volume of Stock Traded")
    dtFmt = mdates.DateFormatter('%Y-%m-%d %H:%M')
    axes[1].xaxis.set_major_formatter(dtFmt)
    axes[1].xaxis.set_tick_params(rotation=90, labelsize='x-small')
    axes[1].legend(loc='upper left')

    plt.xlabel("timestamp")
    plt.suptitle(title, fontsize=13)
    plt.tight_layout(rect=[0, 0, 1, 0.97])  # Adjust subplot layout
    plt.show()

"""### Let us print for the case of the month of January 2020"""

for i in chosen_stocks:
  df = get_subset_of_dataset(i, start_date='2020-01-01 00:00:00', end_date='2020-01-31 23:59:00')
  visualize_stock_prices_day(df, f"Day to Day Stock Prices for {i}, for the month of January 2020")

"""## 1c) Plotting candlestick chart with volume on secondary y-axis
- We will use the python library mplfinance by matplotlib for plotting candlestick charts
"""

!pip install mplfinance

"""### Let us plot for the first quarter of 2019"""

import pandas as pd
import mplfinance as mpf

def visualize_stock_candlestick(df, title):
    df.set_index('datetime', inplace=True)
    # perform aggregations as before to get daily data
    daily_closing = df.resample('D').agg({'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last', "volume" : "sum"})
    # use mpf.plot() to plot the candlestick plot
    fig, axes = mpf.plot(daily_closing, type='candle', style='yahoo', title=title, ylabel='Price', volume=True, figsize=(15, 8), returnfig=True)
    axes[1].set_ylabel('Volume')
    mpf.show()

for i in chosen_stocks:
  df = get_subset_of_dataset(i, start_date='2020-01-01 00:00:00', end_date='2020-04-30 23:59:00')
  visualize_stock_candlestick(df, f"Candlestick chart for {i}, for the first quarter of 2020")

"""## 1d) Observations of Data

We can see the following things, which have been explained below with some plot image example



"""

plt.style.use('default')
df = get_subset_of_dataset(i, start_date='2020-01-01 00:00:00', end_date='2020-01-31 23:59:00')
visualize_stock_prices_day(df, f"Day to Day Stock Prices for Apple, for the month of January 2020")

"""- See above figure : There are a few days for which the data is missing, for example in the case of study of a stock for a month, we will see abrupt jumps, indicating missing data for some days (this may be because of holidays or days when the market was not functioning)
- While training the LSTM, we will ignore these jumps and consider it to be a single continuous time series

## 2) Normalizing Stock Data
Two ways in which we can normalize stock data is using
- Min-Max Scaling : the relative relationships between stock prices and volumes fall within the same range, thus we can compare stocks across different companies and time periods

- Z score Scaling : we transform data to have a mean of 0 and a standard deviation of 1, centering the data around a common mean and express values in terms of standard deviations from the mean.

## Strategy we will use for Normalization

- We will use normalizers from sklearn.preprocessing, we will need seperate normalizers for volume and stock prices
- Let us take the data for the first quarter of 2020
"""

mm_company_wise = {}

for i in chosen_stocks:
  # get a company wise dictionary of datasets
  mm_company_wise[i] = get_subset_of_dataset(i, start_date='2020-01-01 00:00:00', end_date='2020-04-30 23:59:00')

# combine all company data together to perform normalization
combined_data = pd.concat(list(mm_company_wise.values()), axis=0, ignore_index=True)
# we will perform seperate
price_columns = ['open', 'high', 'low', 'close']
volume_columns = ['volume']

from sklearn.preprocessing import MinMaxScaler

mm_price_scaler = MinMaxScaler()
mm_volume_scaler = MinMaxScaler()

# fit the scaler on the combined data
mm_price_scaler.fit(combined_data[price_columns])
mm_volume_scaler.fit(combined_data[volume_columns])

for i in chosen_stocks:
  # after fitting, transform the data
  mm_company_wise[i][['open', 'close', 'low', 'high']] = mm_price_scaler.fit_transform(mm_company_wise[i][['open', 'close', 'low', 'high']])
  mm_company_wise[i]['volume'] = mm_volume_scaler.fit_transform(mm_company_wise[i][['volume']])
  visualize_stock_prices_day(mm_company_wise[i], f"MinMax Scored Scaled Datapoints for {i}, for the first quarter of 2020")

# perform the same as MinMax for StandardScalar for z-score based normalization
z_company_wise = {}

for i in chosen_stocks:
  z_company_wise[i] = get_subset_of_dataset(i, start_date='2020-01-01 00:00:00', end_date='2020-04-30 23:59:00')

from sklearn.preprocessing import StandardScaler

z_price_scaler = StandardScaler()
z_volume_scaler = StandardScaler()

z_price_scaler.fit(combined_data[price_columns])
z_volume_scaler.fit(combined_data[volume_columns])

for i in chosen_stocks:
  z_company_wise[i][['open', 'close', 'low', 'high']] = z_price_scaler.fit_transform(z_company_wise[i][['open', 'close', 'low', 'high']])
  z_company_wise[i]['volume'] = z_volume_scaler.fit_transform(z_company_wise[i][['volume']])
  visualize_stock_prices_day(z_company_wise[i], f"Z-score Scaled Datapoints for {i}, for the first quarter of 2020")

"""## 2) Methodology for choosing the Scaling Method for various Scenarios

- For high-frequency trading (intra-day-trading), price and volume can fluctuate rapidly, in such cases Z-score would be more better than MaxMin, as it scales on the basis of mean and standard deviation and can capture short term variations better (Z-score better in this case)

- For inter-day or long term, MinMax scaling would be better, since this scaling maintains the relative differences stable for longer time periods (MinMax better in this case)

- If we allow trading simulation to account for buy-ask spreads (related to volume and price) and commissions then Z-score normalization can keep the relative significance of spreads and commissions, while Min-Max scaling cannot account for this information
(Z-score better in this case)

- Z-score normalization makes it easier to compare the relative importance of price and volume changes across different assets.

### **Seeing that the advantages of using Z-score Normalization outweigh for the application which we plan to target (see below), we will proceed with Z-score normalization**

## 3) Final Assumption Decisions
- ### **We will perform High Frequency Trading (Intra-day) where we trade after a said time of $m$ minutes (for example once every 10 minutes)**
- ### **Using one/some/all (we can try all one-by-one) stocks from the Tech basket (which is why DELL, CSCO, ADBE, AAPL, AMZN, AMD were chosen)**
  - We will predict AAPL stock price only using AAPL closing price, AAPL all parameters (close, open, high, low, volume) and also another example (see Q9) where we consider correlated stocks AAPL and AMD  
- ### **After a quick research online, we assume buy-ask spread and commissions for each trade as the following**
  - The commission per trade is **1 USD** (it is lower for HFT than daily since the frequency of trade is higher in this case) and the buy ask spread is **5 cents** (average values used for high frequency trading in NYSE as per the internet)

## 4) PyTorch Module LSTM  
[Reference Implementation Followed](https://www.kaggle.com/code/rodsaldanha/stock-prediction-pytorch?scriptVersionId=35329420&cellId=11) : Credits to Ron Saldanha (Kaggle)
"""

import torch
import torch.nn as nn

# the torch implementation of LSTM : reference used https://www.kaggle.com/code/rodsaldanha/stock-prediction-pytorch?scriptVersionId=35329420&cellId=11

class LSTM(nn.Module):

    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):
        super(LSTM, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()
        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))
        out = self.fc(out[:, -1, :])
        return out

"""## Strategy to combine various company stock data together
- We shall make use of inner joins on the key for example in the bottom case we take common keys of datetime for both stocks to prepare the dataset
"""

df1 = get_subset_of_dataset('AAPL', start_date='2020-01-06 00:00:00', end_date='2020-01-06 9:00:00')
df1.set_index('datetime', inplace=True)

df2 = get_subset_of_dataset('AMZN', start_date='2020-01-06 00:00:00', end_date='2020-01-06 9:00:00')
df2.set_index('datetime', inplace=True)

df1.head()

df2.head()

df1 = df1.merge(df2, how='inner', on='datetime') # merge the two dataframes using inner joins to combine two company data

df1.head()

"""Thus we can combine in such a way that the stock price which we want to predict is right in front or in the first column of the dataframe

## Strategy to create variable window dataset for LSTM
- Input : k windowed vector $(x_{t-k}, ... x_{t-2}, x_{t-1})$
- Output : next time step $y = x_t$
"""

df = z_company_wise['AAPL'][['close', 'open', 'high', 'low', 'volume']]
df.head()

df_np = df.to_numpy()
df_np[:5, :]

data = []
window_size = 4
for index in range(len(df_np) - window_size):
  # generate a k-size windowed input
  data.append(df_np[index : index + window_size])
data = np.array(data)
x_train = data[:1,:-1,:] # input data
y_train = data[:1,-1,0].reshape(-1, 1) # output data we predict the first column generally, here it is closing price

x_train

y_train

"""## 5) Flexible Dataloader for training the LSTM"""

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

import torch
from torch.utils.data import Dataset, DataLoader

class TimeSeriesDataset(Dataset):
    # we will use this class to store our timeseries data (this extends nn's native dataset class)
    def __init__(self, data, targets, transform=None):
        self.data = data
        self.targets = targets
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sample = {'data': self.data[idx], 'target': self.targets[idx]}

        if self.transform:
            sample = self.transform(sample)

        return sample

class LSTMTrainDataLoader():
  def __init__(self, companies, start_timestamp, end_timestamp):
    # takes the set of companies we should consider while creating dataset
    # also takes the start timestamp and end timestamp
    self.companies = companies
    self.z_company_wise = {}
    # we perform z-score based normalization as done in Q2
    # two seperate scalers, one for prices and the other for volume
    self.z_price_scaler = StandardScaler()
    self.z_volume_scaler = StandardScaler()

    for i in companies:
      z_company_wise[i] = get_subset_of_dataset(i, start_date=start_timestamp, end_date=end_timestamp)
      z_company_wise[i].set_index('datetime', inplace=True)

    combined_data = pd.concat(list(z_company_wise.values()), axis=0, ignore_index=True)
    price_columns = ['open', 'high', 'low', 'close']
    volume_columns = ['volume']

    # fit the scaler on concatenated data
    z_price_scaler.fit(combined_data[price_columns])
    z_volume_scaler.fit(combined_data[volume_columns])

    # transform the scaled data
    for i in companies:
      z_company_wise[i][['open', 'close', 'low', 'high']] = z_price_scaler.fit_transform(z_company_wise[i][['open', 'close', 'low', 'high']])
      z_company_wise[i]['volume'] = z_volume_scaler.fit_transform(z_company_wise[i][['volume']])


  def generate_dataset(self, company, other_companies_to_include = [], input_rows = ['close'], train_ratio=0.7, validation_ratio=0.15, test_ratio=0.15, window_size=20, batch_size=32, shuffle=False):
    # first column name in input_rows should be the stock which we want to predict, generally close
    # company : the main company whose stock price we wish to predict
    # only take the input rows
    # window_size : this decides the number of past inputs to be included in the window
    # batch_size : this is to decide the size of each batch of the dataloader
    # shuffle : this is to decide that the dataset must be shuffled or not
    df = z_company_wise[company][input_rows]

    # if we have some other company which we wish to include as in the case of Q9
    # for multiple stock price as input to pre
    if len(other_companies_to_include) > 0:
      for comp in other_companies_to_include:
        new_df = z_company_wise[comp][input_rows]
        # perform inner join as per discussed strategy
        df = df.merge(new_df, how='inner', on='datetime')

    df_np = df.to_numpy()
    print(df_np.shape)
    data = []

    # perform the windowed dataset generation
    for index in range(len(df_np) - window_size):
        data.append(df_np[index : index + window_size])

    data = np.array(data)

    # identify the splits for test, train and validation dataset
    # by default we perform 70 : 15 : 15 split
    test_set_size = int(np.round(test_ratio*data.shape[0]))
    validation_set_size = int(np.round(validation_ratio*data.shape[0]))
    train_set_size = data.shape[0] - (test_set_size) - (validation_set_size)

    # divide the dataset into test, train, validation
    x_train = data[:train_set_size,:-1,:]
    y_train = data[:train_set_size,-1,0].reshape(-1, 1)

    x_test = data[train_set_size: train_set_size + validation_set_size,:-1,:]
    y_test = data[train_set_size: train_set_size + validation_set_size,-1,0].reshape(-1, 1)

    x_valid = data[train_set_size + validation_set_size : ,:-1, :]
    y_valid = data[train_set_size + validation_set_size : ,-1,0].reshape(-1, 1)

    # convert to torch tensors
    self.x_train = torch.from_numpy(x_train).type(torch.Tensor)
    self.x_test = torch.from_numpy(x_test).type(torch.Tensor)
    self.y_train = torch.from_numpy(y_train).type(torch.Tensor)
    self.y_test = torch.from_numpy(y_test).type(torch.Tensor)
    self.x_valid = torch.from_numpy(x_valid).type(torch.Tensor)
    self.y_valid = torch.from_numpy(y_valid).type(torch.Tensor)

    # create torch datasets
    self.train_data = TimeSeriesDataset(self.x_train, self.y_train)
    self.test_data = TimeSeriesDataset(self.x_test, self.y_test)
    self.valid_data = TimeSeriesDataset(self.x_valid, self.y_valid)

    # print shapes for easier debugging
    print("X : train_shape:",x_train.shape, "test_shape:", x_test.shape, "valid_shape:", x_valid.shape)
    print("Y : train_shape", y_train.shape, "test_shape:", y_test.shape, "valid_shape:", y_valid.shape)

    # generate dataloader using torch's DataLoader class with shuffle and batch size as decided
    self.train_dataloader = DataLoader(self.train_data, batch_size=batch_size, shuffle=shuffle)
    self.test_dataloader = DataLoader(self.test_data, batch_size=batch_size, shuffle=shuffle)
    self.valid_dataloader = DataLoader(self.valid_data, batch_size=batch_size, shuffle=shuffle)
    print("Dataloader Created Successfully")

"""### Some Examples"""

dataloader = LSTMTrainDataLoader(['AAPL'], '2019-01-01 00:00:00', '2019-01-06 23:59:59')
dataloader.generate_dataset('AAPL', [], ['close', 'open'])

dataloader = LSTMTrainDataLoader(['AAPL', 'AMZN'], '2019-01-01 00:00:00', '2019-01-06 23:59:59')
dataloader.generate_dataset('AAPL', [], ['close', 'open', 'high', 'low'])

dataloader = LSTMTrainDataLoader(['AAPL', 'AMZN'], '2019-01-01 00:00:00', '2019-01-06 23:59:59')
dataloader.generate_dataset('AAPL', ['AMZN'], ['close'])

dataloader = LSTMTrainDataLoader(['AAPL', 'AMZN'], '2019-01-01 00:00:00', '2019-01-06 23:59:59')
dataloader.generate_dataset('AAPL', ['AMZN'], ['close', 'open', 'high', 'low'])

"""## 6) Training the LSTM

### Predicting Close Price for Single Company (AAPL) only using its Close Price as input
- We will train the data on the data for a quarter (4 months) and predict it for the next quarter
"""

input_dim = 1
hidden_dim = 32
num_layers = 2
output_dim = 1
num_epochs = 100
batch_size = 64

model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)
criterion = torch.nn.MSELoss(reduction='mean')
optimiser = torch.optim.Adagrad(model.parameters(), lr=0.005)
model

dataloader = LSTMTrainDataLoader(['AAPL'], '2020-01-01 00:00:00', '2020-04-30 23:59:59')

dataloader.generate_dataset('AAPL', [], ['close'], batch_size=batch_size, shuffle=True, window_size=25)

import time

def train_model(model, num_epochs, dataloader, patience):
  hist = np.zeros(num_epochs) # to store the training loss to plot it later
  start_time = time.time()
  prev_time = start_time
  best_val_loss = np.inf # to store minimum value of validation loss for early stopping


  train_loader = dataloader.train_dataloader
  valid_loader = dataloader.valid_dataloader

  for t in range(num_epochs):
      train_loss = 0.0 # running sum of train loss
      for batch in train_loader:
        # predict for each training batch
        y_train_pred = model(batch['data'])
        # evaluate loss
        loss = criterion(y_train_pred, batch['target'])
        optimiser.zero_grad()
        # perform back propogation
        loss.backward()
        optimiser.step()
        train_loss += loss.item()
      # evaluate average train loss for the epoch
      train_loss /= len(train_loader)

      # evaluate validation loss
      val_loss = 0.0
      for batch in valid_loader:
        y_val_pred = model(batch['data'])
        loss = criterion(y_val_pred, batch['target'])
        val_loss += loss.item()
      val_loss /= len(train_loader)

      curr_time = time.time()
      print("Epoch ", t, "Train MSE: ", train_loss, "Validation MSE: ", val_loss, "Time taken for Epoch:", curr_time - prev_time)
      prev_time = curr_time
      hist[t] = train_loss

      if val_loss < best_val_loss:
          # check if current val loss is lesser than minimum
          best_val_loss = val_loss
          no_improvement_count = 0
      else:
          no_improvement_count += 1

      if no_improvement_count >= patience:
          # if there is no improvement in val loss for patience epochs, then perform early stopping
          print(f'Early stopping after {patience} epochs of no improvement.')
          break

  training_time = time.time()-start_time
  print("Training time: {}".format(training_time))
  return hist

hist = train_model(model, num_epochs, dataloader, 10)

torch.save(model, 'single_feature.pt')

import seaborn as sns

def visualize_train_results(model, dataloader, title, ylabel):
  # get the predicted value
  y_train_pred_np = model(dataloader.x_train).detach().numpy()
  bv = np.ones((1, 4))
  # broadcast because our scaler takes a 4 dim vector (corresponding to each price), we ultimately will take one column from here
  y_tp_rshp = y_train_pred_np * bv

  # get the actual value
  y_train_og_np = dataloader.y_train.detach().numpy()
  y_to_rshp = y_train_og_np * bv

  predict = pd.DataFrame(z_price_scaler.inverse_transform(y_tp_rshp))
  original = pd.DataFrame(z_price_scaler.inverse_transform(y_to_rshp))

  sns.set_style("darkgrid")

  fig = plt.figure()
  fig.subplots_adjust(hspace=0.2, wspace=0.2)

  plt.subplot(1, 2, 1)
  # index three because our scaler had closing price at fourth position while training
  ax = sns.lineplot(x = original[3].index, y = original[3], label="Data", color='royalblue')
  ax = sns.lineplot(x = predict[3].index, y = predict[3], label="Training Prediction (LSTM)", color='tomato')
  ax.set_title(title, size = 14, fontweight='bold')
  ax.set_xlabel("Minute", size = 14)
  ax.set_ylabel(ylabel, size = 14)
  ax.set_xticklabels('', size=10)


  plt.subplot(1, 2, 2)
  ax = sns.lineplot(data=hist, color='royalblue')
  ax.set_xlabel("Epoch", size = 14)
  ax.set_ylabel("Loss", size = 14)
  ax.set_title("Training Loss", size = 14, fontweight='bold')
  fig.set_figheight(6)
  fig.set_figwidth(16)

visualize_train_results(model, dataloader, "Apple Stock Price", "Closing Price")

def visualize_test_results(model, dataloader, title, ylabel):
  # perform the same as visualize_test_results() method but using the test data
  y_test_pred = model(dataloader.x_test)
  y_test_pred_np = y_test_pred.detach().numpy()
  bv = np.ones((1, 4))
  y_tp_rshp = y_test_pred_np * bv

  y_test_og_np = dataloader.y_test.detach().numpy()
  y_to_rshp = y_test_og_np * bv

  print(f"Testing MSE Loss : {criterion(y_test_pred, dataloader.y_test)}")

  predict = pd.DataFrame(z_price_scaler.inverse_transform(y_tp_rshp))
  original = pd.DataFrame(z_price_scaler.inverse_transform(y_to_rshp))

  sns.set_style("darkgrid")

  fig = plt.figure(figsize=(20, 10))

  sns.lineplot(x = original[3].index, y = original[3], label="Actual Data", color='royalblue')
  sns.lineplot(x = predict[3].index, y = predict[3], label="Test Prediction (LSTM)", color='tomato')
  plt.title(title, size = 14, fontweight='bold')
  plt.xlabel("Minute", size = 14)
  plt.ylabel(ylabel, size = 14)

  fig.set_figheight(8)
  fig.set_figwidth(18)

visualize_test_results(model, dataloader, "Apple Stock Price", "Closing Price")

"""### Class for Prediction Data Loader
- We will predict using the data trained on previous quarter for the next quarter
"""

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

class LSTMPredictDataLoader():
  # we will use this class to store the prediction data which the model has not seen
  def __init__(self, companies, start_timestamp, end_timestamp):
    self.companies = companies
    self.z_company_wise = {}
    self.z_price_scaler = StandardScaler()
    self.z_volume_scaler = StandardScaler()

    for i in companies:
      z_company_wise[i] = get_subset_of_dataset(i, start_date=start_timestamp, end_date=end_timestamp)
      z_company_wise[i].set_index('datetime', inplace=True)

    combined_data = pd.concat(list(z_company_wise.values()), axis=0, ignore_index=True)
    price_columns = ['open', 'high', 'low', 'close']
    volume_columns = ['volume']

    z_price_scaler.fit(combined_data[price_columns])
    z_volume_scaler.fit(combined_data[volume_columns])

    for i in companies:
      z_company_wise[i][['open', 'close', 'low', 'high']] = z_price_scaler.fit_transform(z_company_wise[i][['open', 'close', 'low', 'high']])
      z_company_wise[i]['volume'] = z_volume_scaler.fit_transform(z_company_wise[i][['volume']])


  def generate_dataset(self, company, other_companies_to_include = [], input_rows = ['close'], window_size=20):
    # first column name in input_rows should be the stock which we want to predict, generally close
    df = z_company_wise[company][input_rows]

    if len(other_companies_to_include) > 0:
      for comp in other_companies_to_include:
        new_df = z_company_wise[comp][input_rows]
        df = df.merge(new_df, how='inner', on='datetime')

    df_np = df.to_numpy()
    data = []

    for index in range(len(df_np) - window_size):
        data.append(df_np[index : index + window_size])

    data = np.array(data)

    # same as training dataloader, no need of test, val, train split
    # since we only use it for prediction

    x_value = data[:,:-1,:]
    y_value = data[:,-1,0].reshape(-1, 1)

    self.x_value = torch.from_numpy(x_value).type(torch.Tensor)
    self.y_value = torch.from_numpy(y_value).type(torch.Tensor)
    print("Dataloader Created Successfully")
    print("X : value_shape:",x_value.shape)
    print("Y : value_shape", y_value.shape)

from sklearn.metrics import mean_absolute_percentage_error

def predict_for_next_timeframe(predict_dataloader, model, minutes, title, ylabel):
  criterion = mean_absolute_percentage_error
  # use the predict dataloader to perform predictions for the next "minutes" number of minutes
  y_test_pred = model(predict_dataloader.x_value[ : minutes])
  y_test_pred_np = y_test_pred.detach().numpy()
  bv = np.ones((1, 4))
  y_tp_rshp = y_test_pred_np * bv

  y_test_og_np = predict_dataloader.y_value[ : minutes].detach().numpy()
  y_to_rshp = y_test_og_np * bv

  mape = criterion(y_test_pred.detach().numpy(), predict_dataloader.y_value[ : minutes].detach().numpy())

  print(f"Mean Absolute Percentage Error : {mape}")

  predict = pd.DataFrame(z_price_scaler.inverse_transform(y_tp_rshp))
  original = pd.DataFrame(z_price_scaler.inverse_transform(y_to_rshp))

  sns.set_style("darkgrid")

  fig = plt.figure(figsize=(20, 10))

  sns.lineplot(x = original[3].index, y = original[3], label="Actual Data", color='royalblue')
  sns.lineplot(x = predict[3].index, y = predict[3], label="Future Prediction (by LSTM)", color='tomato')
  plt.title(title, size = 14, fontweight='bold')
  plt.xlabel("Minutes into the Future", size = 14)
  plt.ylabel(ylabel, size = 14)

  fig.set_figheight(8)
  fig.set_figwidth(18)

  return mape

# get data for next quarter (next four month)
single_predict_dataloader = LSTMPredictDataLoader(['AAPL'], '2020-05-01 00:00:00', '2020-09-30 23:59:59')
single_predict_dataloader.generate_dataset('AAPL', window_size=25)

"""### Predicting Close Price for Single Company (AAPL) using all (Open, Close, High, Low, Volume) as inputs
- We will train the data on the data for a quarter (4 months) and predict it for the next quarter
"""

input_dim = 5
hidden_dim = 32
num_layers = 2
output_dim = 1
num_epochs = 100
batch_size = 64

yr_model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)
criterion = torch.nn.MSELoss(reduction='mean')
optimiser = torch.optim.Adagrad(yr_model.parameters(), lr=0.005)
yr_model

dataloader = LSTMTrainDataLoader(['AAPL'], '2020-01-01 00:00:00', '2020-04-30 23:59:59')

dataloader.generate_dataset('AAPL', [], ['close', 'open', 'high', 'low', 'volume'], batch_size=batch_size, shuffle=True, window_size=25)

hist = train_model(yr_model, num_epochs, dataloader, 10)

torch.save(yr_model, 'multiple_feature.pt')

visualize_train_results(yr_model, dataloader, "Apple Stock Price", "Closing Price")

visualize_test_results(yr_model, dataloader, "Apple Stock Price", "Closing Price")

# get data for next quarter (next four month)
multiple_predict_dataloader = LSTMPredictDataLoader(['AAPL'], '2020-05-01 00:00:00', '2020-09-30 23:59:59')
multiple_predict_dataloader.generate_dataset('AAPL', [], input_rows=['close', 'open', 'high', 'low', 'volume'], window_size=25)

"""### Future Predictions for Single Column Input"""

times = [100, 200, 500, 24*60, 7*24*60, 14*24*60, 30*24*60, 60*24*60]
headers = ['100 minutes', '200 minutes', '500 minutes', '1 day', '1 week', '2 weeks', '1 month', '2 months']
single_mape_vec = []
for j, i in enumerate(times):
  single_mape_vec.append(float(predict_for_next_timeframe(single_predict_dataloader, model, i, f"Predicted Apple Stock Price {headers[j]} into the future", "Closing Price")))

"""### Future Predictions for Multiple Column Input"""

times = [100, 200, 500, 24*60, 7*24*60, 14*24*60, 30*24*60, 60*24*60]
headers = ['100 minutes', '200 minutes', '500 minutes', '1 day', '1 week', '2 weeks', '1 month', '2 months']
multiple_mape_vec = []
for j, i in enumerate(times):
  multiple_mape_vec.append(float(predict_for_next_timeframe(multiple_predict_dataloader, yr_model, i, f"Predicted Apple Stock Price {headers[j]} into the future", "Closing Price")))

"""## 7) Trading Module"""

class AlgorithmicTrader():

  def __init__(self, lstm, initial_money, commission_per_trade, buy_ask_spread):
    self.lstm = lstm
    self.curr_money = initial_money
    self.commission_per_trade = commission_per_trade
    self.buy_ask_spread = buy_ask_spread
    self.curr_minute = 0
    self.number_of_stocks = 0

  def get_net_worth(self, price):
    # returns the total money and monetary value of stocks given current price
    return self.curr_money + (price * self.number_of_stocks)

  def buy(self, num_of_stocks, price, debug):
    if(self.curr_money >= price*num_of_stocks + self.commission_per_trade and self.curr_money != 0):
      # check if sufficient money, if so buy the stocks, else do nothing
      self.number_of_stocks += num_of_stocks
      self.curr_money -= (price*num_of_stocks)
      self.curr_money -= self.commission_per_trade
      if debug:
        print(f"Bought : {num_of_stocks} stocks")
    else:
      if debug:
        print("Holding : Did nothing")

  def sell(self, num_of_stocks, price, debug):
    if(self.number_of_stocks >= num_of_stocks and self.number_of_stocks != 0):
      # check if number of stocks to sell, is more than the number of stocks you have
      self.number_of_stocks -= num_of_stocks
      self.curr_money += (price*num_of_stocks)
      self.curr_money -= self.commission_per_trade
      if debug:
        print(f"Sold : {num_of_stocks} stocks")
    else:
      if debug:
        print("Holding : Did nothing")

  def get_max_stocks_buyable(self, price):
    stocks = 0
    # get maximum number of stocks which can be bought for the price given current amount of money
    while(price*stocks + self.commission_per_trade <= self.curr_money):
      stocks += 1
    return stocks - 1

  def reinitialize_after_training(self):
    # set current minute to zero since, to start indexing from zero
    self.curr_minute = 0

  def trade(self, predict_dataloader, trade_after, max_interval, normal_trade_stocks, min_profit_threshold, max_profit_threshold, debug=False):
    start_time = self.curr_minute
    global_max = 0
    curr_iter = 0
    # check if maximum time interval has passed
    while (self.curr_minute - start_time) < max_interval:
      # perform predictions using model
      predict_minute = self.curr_minute + trade_after
      y_test_pred = self.lstm(predict_dataloader.x_value[predict_minute : predict_minute + 1])
      y_test_pred_np = y_test_pred.detach().numpy()
      bv = np.ones((1, 4))
      y_tp_rshp = y_test_pred_np * bv

      y_test_og_np = predict_dataloader.y_value[predict_minute : predict_minute + 1].detach().numpy()
      y_to_rshp = y_test_og_np * bv

      prediction = z_price_scaler.inverse_transform(y_tp_rshp)[0][3]
      current_price = predict_dataloader.y_value[self.curr_minute : self.curr_minute + 1]
      current_price = current_price.detach().numpy()
      bv = np.ones((1, 4))
      current_price = current_price * bv
      current_price = z_price_scaler.inverse_transform(current_price)[0][3]
      # check if the current price is larger than the global maxima since the start
      global_max = max(global_max, current_price)
      # check the value of predicted profit considering buy ask spread and commission per trade
      pred_profit = (prediction - (current_price + self.buy_ask_spread)) * normal_trade_stocks - self.commission_per_trade

      if curr_iter == 0:
         print("Start Net Worth: ", self.get_net_worth(current_price))

      if debug:
        print("---------------------------------------")
        print(f"Current minute {self.curr_minute}")
        print(f"Global maximal price", global_max)
        print("Current price :", current_price)
        print("Current money :", self.curr_money)
        print("Current stocks :", self.number_of_stocks)
        print(f"Net Worth :", self.get_net_worth(current_price))
        print("Predicted Price :", prediction)
        print("Predicted Profit :", pred_profit)
        print("---------------------------------------\n\n")

      if(pred_profit >= max_profit_threshold and prediction > global_max):
        # check if the predicted price is greater than global maxima, and if the profit predicted is greater than max threshold
        if debug:
          print("Did First Option")
        # buy as many stocks you can for the money which you have
        self.buy(self.get_max_stocks_buyable(current_price + self.buy_ask_spread), current_price + self.buy_ask_spread, debug)
      elif(pred_profit >= min_profit_threshold and prediction > global_max):
        # check if the predicted price is greater than global maxima, and if the profit predicted is greater than min threshold
        if debug:
          print("Did Second Option")
        # buy the normal number of stocks which you buy regularly
        self.buy(normal_trade_stocks, current_price + self.buy_ask_spread, debug)
      else:
        # otherwise sell all the stocks that you own
        if debug:
          print("Did Last Option")
        self.sell(self.number_of_stocks, current_price - self.buy_ask_spread, debug)

      self.curr_minute = self.curr_minute + trade_after
      curr_iter += 1
    print("End Net Worth: ", self.get_net_worth(current_price))

class BuyAndHoldTrader():
  # the module which does a normal buy and sell after said time period
  def __init__(self, initial_money, commission_per_trade, buy_ask_spread):
    self.curr_money = initial_money
    self.commission_per_trade = commission_per_trade
    self.buy_ask_spread = buy_ask_spread
    self.curr_minute = 0
    self.number_of_stocks = 0

  def get_net_worth(self, price):
    return self.curr_money + (price * self.number_of_stocks)

  def buy(self, num_of_stocks, price, debug):
    if(self.curr_money >= price*num_of_stocks + self.commission_per_trade and self.curr_money != 0):
      self.number_of_stocks += num_of_stocks
      self.curr_money -= (price*num_of_stocks)
      self.curr_money -= self.commission_per_trade
      if debug:
        print(f"Bought : {num_of_stocks} stocks")
    else:
      if debug:
        print("Holding : Did nothing")

  def sell(self, num_of_stocks, price, debug):
    if(self.number_of_stocks >= num_of_stocks and self.number_of_stocks != 0):
      self.number_of_stocks -= num_of_stocks
      self.curr_money += (price*num_of_stocks)
      self.curr_money -= self.commission_per_trade
      if debug:
        print(f"Sold : {num_of_stocks} stocks")
    else:
      if debug:
        print("Holding : Did nothing")

  def get_max_stocks_buyable(self, price):
    stocks = 0
    while(price*stocks + self.commission_per_trade <= self.curr_money):
      stocks += 1
    return stocks - 1

  def trade(self, predict_dataloader, max_interval, debug=False):
    sell_minute = self.curr_minute + max_interval
    bv = np.ones((1, 4))
    # get price for selling minute
    y_test_og_np = predict_dataloader.y_value[sell_minute : sell_minute + 1].detach().numpy()
    y_to_rshp = y_test_og_np * bv

    sell_price = z_price_scaler.inverse_transform(y_to_rshp)[0][3]
    # get current price
    current_price = predict_dataloader.y_value[self.curr_minute : self.curr_minute + 1]
    current_price = current_price.detach().numpy()
    bv = np.ones((1, 4))
    current_price = current_price * bv
    current_price = z_price_scaler.inverse_transform(current_price)[0][3]

    print("Start Net Worth: ", self.get_net_worth(current_price))
    # buy as many stocks with the money for current price
    self.buy(self.get_max_stocks_buyable(current_price + self.buy_ask_spread), current_price + self.buy_ask_spread, debug)
    # sell all the stocks for the selling price
    self.sell(self.number_of_stocks, sell_price - self.buy_ask_spread, debug)
    print("End Net Worth: ", self.get_net_worth(sell_price))

    self.curr_minute = sell_minute

"""### With no commission and buy-ask spread

### The Single Input Feature LSTM
"""

algo_trader = AlgorithmicTrader(model, 10000, 0, 0)

algo_trader.trade(single_predict_dataloader, 10, 1*24*60, 5, 0.3, 0.5, False)

"""### The Multiple Input Feature LSTM"""

algo_trader_m = AlgorithmicTrader(yr_model, 10000, 0, 0)

algo_trader_m.trade(multiple_predict_dataloader, 10, 1*24*60, 5, 0.3, 0.5, False)

"""### Simple Buy and Hold Strategy"""

normal_trader = BuyAndHoldTrader(10000, 0, 0)

normal_trader.trade(single_predict_dataloader, 1*24*60, True)

"""### With commission and buy-ask spread

Assuming that the commission per trade is 1 USD and the buy ask spread is 5 cents (average values used for high frequency trading in NYSE as per the internet) as discussed above in Q3

### The Single Input Feature LSTM
"""

algo_trader = AlgorithmicTrader(model, 10000, 1, 0.05)

algo_trader.trade(single_predict_dataloader, 10, 1*24*60, 5, 0.3, 0.5, False)

"""### The Multiple Input Feature LSTM"""

algo_trader_m = AlgorithmicTrader(yr_model, 10000, 1, 0.05)

algo_trader_m.trade(multiple_predict_dataloader, 10, 1*24*60, 5, 0.3, 0.5, False)

"""### Simple Buy and Hold Strategy"""

normal_trader = BuyAndHoldTrader(10000, 1, 0.05)

normal_trader.trade(single_predict_dataloader, 1*24*60, True)

"""## 8) Testing the Trading Module

### 8a) Plotting Prediction Error from time last trained

### Prediction comparison for single input vs multiple inputs to the LSTM
"""

def visualize_mape(headers, *mape_vecs):
    x = np.array(range(len(headers)))
    plt.figure(figsize=(14, 5))
    plt.bar(x, mape_vecs[0], label='single input (close)', color='blue', alpha=0.7, width=0.1)
    plt.bar(x + 0.1, mape_vecs[1], label='multiple input (close, open, high, low, volume)', color='red', alpha=0.7, width=0.1)
    # Adding labels and title
    plt.xticks(x, headers)
    plt.xlabel('Minutes into the Future')
    plt.ylabel('Price Prediction Error (MAPE)')
    plt.title('Comparison of various methods')
    plt.legend()

    # Adjusting the layout for better visibility
    plt.tight_layout()

    # Show the plot
    plt.show()

visualize_mape(headers, single_mape_vec, multiple_mape_vec)

"""Thus we see that the prediction error is less when we include more number of inputs, and the LSTM performs better and have lesser MAPE for more inputs, but becomes worse as the time from not being trained increases to greater than a month

We see that the error is minimal for short durations of time (upto 1-2 weeks), but becomes progressively larger as the time duration increases upto a month and so on

### To test the training module on a long duration on later years we shall follow the following strategy. We will perform the test for a duration of one year (2021)

- The buy and hold trader will just buy maximum possible stocks, wait for one year and sell the stocks

- We saw from the MAPEs evaluated that LSTM starts to perform poorly as the timeframe exceeds more 2 months, hence we shall perform retraining on the passed quarter once every 4 months

- To maximize retention of stock price knowledge, algorithmic trader will train on every four months data and predict for the next 4 months and use the LSTM to predict prices

### Buy and Hold Trader
"""

# get data for next year
bh_predict_dataloader = LSTMPredictDataLoader(['AAPL'], '2021-01-01 00:00:00', '2022-01-31 23:59:59')
bh_predict_dataloader.generate_dataset('AAPL', window_size=25)

normal_trader = BuyAndHoldTrader(10000, 0, 0)
normal_trader.trade(bh_predict_dataloader, 224357, True)

cnormal_trader = BuyAndHoldTrader(10000, 1, 0.05)
cnormal_trader.trade(bh_predict_dataloader, 224357, True)

"""### Algorithmic Trader"""

# get training dataloaders in quarters
at_dataloader_1 = LSTMTrainDataLoader(['AAPL'], '2020-09-01 00:00:00', '2020-12-31 23:59:59')
at_dataloader_1.generate_dataset('AAPL', [], ['close', 'open', 'high', 'low', 'volume'], batch_size=batch_size, shuffle=True, window_size=25)
at_dataloader_2 = LSTMTrainDataLoader(['AAPL'], '2021-01-01 00:00:00', '2021-04-30 23:59:59')
at_dataloader_2.generate_dataset('AAPL', [], ['close', 'open', 'high', 'low', 'volume'], batch_size=batch_size, shuffle=True, window_size=25)
at_dataloader_3 = LSTMTrainDataLoader(['AAPL'], '2021-05-01 00:00:00', '2021-08-30 23:59:59')
at_dataloader_3.generate_dataset('AAPL', [], ['close', 'open', 'high', 'low', 'volume'], batch_size=batch_size, shuffle=True, window_size=25)

at_predict_dataloader_1 = LSTMPredictDataLoader(['AAPL'], '2021-01-01 00:00:00', '2021-04-30 23:59:59')
at_predict_dataloader_1.generate_dataset('AAPL', [], input_rows=['close', 'open', 'high', 'low', 'volume'], window_size=25)
at_predict_dataloader_2 = LSTMPredictDataLoader(['AAPL'], '2021-05-01 00:00:00', '2021-08-30 23:59:59')
at_predict_dataloader_2.generate_dataset('AAPL', [], input_rows=['close', 'open', 'high', 'low', 'volume'], window_size=25)
at_predict_dataloader_3 = LSTMPredictDataLoader(['AAPL'], '2021-09-01 00:00:00', '2021-12-31 23:59:59')
at_predict_dataloader_3.generate_dataset('AAPL', [], input_rows=['close', 'open', 'high', 'low', 'volume'], window_size=25)

input_dim = 5
hidden_dim = 32
num_layers = 2
output_dim = 1
num_epochs = 100
batch_size = 64

at_model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)
criterion = torch.nn.MSELoss(reduction='mean')
optimiser = torch.optim.Adagrad(at_model.parameters(), lr=0.005)
at_model

algo_trader = AlgorithmicTrader(at_model, 10000, 0, 0)
calgo_trader = AlgorithmicTrader(at_model, 10000, 1, 0.05)

hist = train_model(at_model, num_epochs, at_dataloader_1, 10)

algo_trader.trade(at_predict_dataloader_1, 10, 71180, 5, 0.3, 0.5, False)
calgo_trader.trade(at_predict_dataloader_1, 10, 71180, 5, 0.3, 0.5, False)

hist = train_model(at_model, num_epochs, at_dataloader_2, 10)

algo_trader.reinitialize_after_training()
calgo_trader.reinitialize_after_training()

algo_trader.trade(at_predict_dataloader_2, 10, 66310, 5, 0.3, 0.5, False)
calgo_trader.trade(at_predict_dataloader_2, 10, 66310, 5, 0.3, 0.5, False)

hist = train_model(at_model, num_epochs, at_dataloader_3, 10)

algo_trader.reinitialize_after_training()
calgo_trader.reinitialize_after_training()

algo_trader.trade(at_predict_dataloader_3, 10, 68770, 5, 0.3, 0.5, False)
calgo_trader.trade(at_predict_dataloader_3, 10, 68770, 5, 0.3, 0.5, False)

"""### Thus the net-worth after performing the four possible trading methods can be ordered as follows
### $net-worth_{algo-without-charges} \ge net-worth_{algo-with-charges} \ge net-worth_{buy-hold-without-charges} \ge net-worth_{buy-hold-with-charges}$

## 8b) Thus from the above analysis, we can conclude the following
- Trading with commission and buy ask spreads is not as profitable as without having them
- However, we see that it is still profitable to trade using the LSTM based trading strategy since we are generating sufficiently large profits even though it is a form of high frequency trading and the number of trades will be large.

## 8c) Thus from the above analysis, we can conclude the following
- The algorithmic trading which makes use of LSTM trained on the data of the previous quarter (4 months) performs exceedingly better than the regular buy and hold strategy, even in the presence of commission and buy ask spread

# Advanced (Bonus)

## 9a) Using multiple stock prices (AMD + AAPL) to predict the prices of AAPL
- We will provide all the inputs (close, open, high, low and volume) and use same parameters as in Q6 to allow comparison
"""

ms_dataloader = LSTMTrainDataLoader(['AAPL', 'AMD'], '2020-01-01 00:00:00', '2020-06-30 23:59:59')
ms_dataloader.generate_dataset('AAPL', ['AMD'], ['close', 'open', 'high', 'low', 'volume'], batch_size=batch_size, shuffle=True, window_size=25)

input_dim = 10
hidden_dim = 32
num_layers = 2
output_dim = 1
num_epochs = 100
batch_size = 64

ms_model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)
criterion = torch.nn.MSELoss(reduction='mean')
optimiser = torch.optim.Adagrad(ms_model.parameters(), lr=0.005)
ms_model

hist = train_model(ms_model, num_epochs, ms_dataloader, 10)

torch.save(ms_model, 'multiple_stock_multiple_feature.pt')

visualize_train_results(ms_model, ms_dataloader, "Apple Stock Price", "Closing Price")

visualize_test_results(ms_model, ms_dataloader, "Apple Stock Price", "Closing Price")

# get data for next quarter (next four month)
ms_predict_dataloader = LSTMPredictDataLoader(['AAPL', 'AMD'], '2020-07-01 00:00:00', '2020-12-31 23:59:59')
ms_predict_dataloader.generate_dataset('AAPL', ['AMD'], input_rows=['close', 'open', 'high', 'low', 'volume'], window_size=25)

"""### Future Predictions for Multiple Stock Analysis (AMD + AAPL)"""

times = [100, 200, 500, 24*60, 7*24*60, 14*24*60, 30*24*60, 58*24*60]
headers = ['100 minutes', '200 minutes', '500 minutes', '1 day', '1 week', '2 weeks', '1 month', '2 months']
ms_mape_vec = []
for j, i in enumerate(times):
  ms_mape_vec.append(float(predict_for_next_timeframe(ms_predict_dataloader, ms_model, i, f"Predicted Apple Stock Price {headers[j]} into the future", "Closing Price")))

"""### Comparisons with other methods done so far"""

def visualize_mape(headers, *mape_vecs):
    x = np.array(range(len(headers)))
    plt.figure(figsize=(14, 5))
    plt.bar(x, mape_vecs[0], label='single input (close)', color='blue', alpha=0.7, width=0.1)
    plt.bar(x + 0.1, mape_vecs[1], label='multiple input (close, open, high, low, volume)', color='red', alpha=0.7, width=0.1)
    plt.bar(x + 0.2, mape_vecs[2], label='multiple stock (AMD + AAPL), multiple input (close, open, high, low, volume)', color='green', alpha=0.7, width=0.1)

    # Adding labels and title
    plt.xticks(x, headers)
    plt.xlabel('Minutes into the Future')
    plt.ylabel('Price Prediction Error (MAPE)')
    plt.title('Comparison of various methods')
    plt.legend()

    # Adjusting the layout for better visibility
    plt.tight_layout()

    # Show the plot
    plt.show()

visualize_mape(headers, ms_mape_vec, single_mape_vec, multiple_mape_vec)

"""### 9a) Inferences
- Thus we can infer that adding more than one stock leads to much more reduced error as compared to just a single input (closing price) and performs better (blue vs green)
- It is not as effective as just using the only single multiple input prices of the stock (red vs green)

#### This may be because of the loss of some data, which occurs when we try to join the two stock price data during pre-processing to synchronize the two stock prices. However the correlation of the two stocks allows it to predict much better than a single input

## 9b) Adding Day, Week and Time as inputs to see if improvement in predictions

We create a modified dataloader for training the LSTM in this case to check if adding these features improves prediction

### Strategy used to add new date time columns to the dataframe
"""

df = z_company_wise['AAPL'][['close', 'open', 'high', 'low', 'volume']]
df.head()

df1 = df.copy()
df1.loc[:, 'day'] = df.index.day
df1.loc[:, 'week'] = df.index.isocalendar().week
df1.loc[:, 'min'] = df.index.minute
df1.loc[:, 'hour'] = df.index.hour
df1.head()

# we perform normalization
df1['day'] = df1['day'] / 31
df1['week'] = df1['week'] / 52
df1['min'] = df1['min'] / 60
df1['hour'] = df1['hour'] / 24
df1.head()

df1.to_numpy().astype(float)[:5]

"""### Modified Dataloader to include Date/Time columns"""

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

import torch
from torch.utils.data import Dataset, DataLoader

class TimeSeriesDataset(Dataset):
    def __init__(self, data, targets, transform=None):
        self.data = data
        self.targets = targets
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sample = {'data': self.data[idx], 'target': self.targets[idx]}

        if self.transform:
            sample = self.transform(sample)

        return sample

class LSTMTrainDataLoaderWithTime():
  def __init__(self, companies, start_timestamp, end_timestamp):
    self.companies = companies
    self.z_company_wise = {}
    self.z_price_scaler = StandardScaler()
    self.z_volume_scaler = StandardScaler()

    for i in companies:
      z_company_wise[i] = get_subset_of_dataset(i, start_date=start_timestamp, end_date=end_timestamp)
      z_company_wise[i].set_index('datetime', inplace=True)

    combined_data = pd.concat(list(z_company_wise.values()), axis=0, ignore_index=True)
    price_columns = ['open', 'high', 'low', 'close']
    volume_columns = ['volume']

    z_price_scaler.fit(combined_data[price_columns])
    z_volume_scaler.fit(combined_data[volume_columns])

    for i in companies:
      z_company_wise[i][['open', 'close', 'low', 'high']] = z_price_scaler.fit_transform(z_company_wise[i][['open', 'close', 'low', 'high']])
      z_company_wise[i]['volume'] = z_volume_scaler.fit_transform(z_company_wise[i][['volume']])


  def generate_dataset(self, company, other_companies_to_include = [], input_rows = ['close'], train_ratio=0.7, validation_ratio=0.15, test_ratio=0.15, window_size=20, batch_size=32, shuffle=False):
    # first column name in input_rows should be the stock which we want to predict, generally close
    df1 = z_company_wise[company][input_rows]

    if len(other_companies_to_include) > 0:
      for comp in other_companies_to_include:
        new_df = z_company_wise[comp][input_rows]
        df1 = df1.merge(new_df, how='inner', on='datetime')

    df = df1.copy()

    df.loc[:, 'day'] = df.index.day
    df.loc[:, 'week'] = df.index.isocalendar().week
    df.loc[:, 'min'] = df.index.minute
    df.loc[:, 'hour'] = df.index.hour
    df['day'] = df['day'] / 31
    df['week'] = df['week'] / 52
    df['min'] = df['min'] / 60
    df['hour'] = df['hour'] / 24

    df_np = df.to_numpy().astype(float)
    print(df_np.shape)
    data = []

    for index in range(len(df_np) - window_size):
        data.append(df_np[index : index + window_size])

    data = np.array(data)
    test_set_size = int(np.round(test_ratio*data.shape[0]))
    validation_set_size = int(np.round(validation_ratio*data.shape[0]))
    train_set_size = data.shape[0] - (test_set_size) - (validation_set_size)

    x_train = data[:train_set_size,:-1,:]
    y_train = data[:train_set_size,-1,0].reshape(-1, 1)

    x_test = data[train_set_size: train_set_size + validation_set_size,:-1,:]
    y_test = data[train_set_size: train_set_size + validation_set_size,-1,0].reshape(-1, 1)

    x_valid = data[train_set_size + validation_set_size : ,:-1, :]
    y_valid = data[train_set_size + validation_set_size : ,-1,0].reshape(-1, 1)

    self.x_train = torch.from_numpy(x_train).type(torch.Tensor)
    self.x_test = torch.from_numpy(x_test).type(torch.Tensor)
    self.y_train = torch.from_numpy(y_train).type(torch.Tensor)
    self.y_test = torch.from_numpy(y_test).type(torch.Tensor)
    self.x_valid = torch.from_numpy(x_valid).type(torch.Tensor)
    self.y_valid = torch.from_numpy(y_valid).type(torch.Tensor)

    self.train_data = TimeSeriesDataset(self.x_train, self.y_train)
    self.test_data = TimeSeriesDataset(self.x_test, self.y_test)
    self.valid_data = TimeSeriesDataset(self.x_valid, self.y_valid)

    print("X : train_shape:",x_train.shape, "test_shape:", x_test.shape, "valid_shape:", x_valid.shape)
    print("Y : train_shape", y_train.shape, "test_shape:", y_test.shape, "valid_shape:", y_valid.shape)

    self.train_dataloader = DataLoader(self.train_data, batch_size=batch_size, shuffle=shuffle)
    self.test_dataloader = DataLoader(self.test_data, batch_size=batch_size, shuffle=shuffle)
    self.valid_dataloader = DataLoader(self.valid_data, batch_size=batch_size, shuffle=shuffle)
    print("Dataloader Created Successfully")

"""### Modified Predict Dataloader"""

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

class LSTMPredictDataLoaderWithTime():

  def __init__(self, companies, start_timestamp, end_timestamp):
    self.companies = companies
    self.z_company_wise = {}
    self.z_price_scaler = StandardScaler()
    self.z_volume_scaler = StandardScaler()

    for i in companies:
      z_company_wise[i] = get_subset_of_dataset(i, start_date=start_timestamp, end_date=end_timestamp)
      z_company_wise[i].set_index('datetime', inplace=True)

    combined_data = pd.concat(list(z_company_wise.values()), axis=0, ignore_index=True)
    price_columns = ['open', 'high', 'low', 'close']
    volume_columns = ['volume']

    z_price_scaler.fit(combined_data[price_columns])
    z_volume_scaler.fit(combined_data[volume_columns])

    for i in companies:
      z_company_wise[i][['open', 'close', 'low', 'high']] = z_price_scaler.fit_transform(z_company_wise[i][['open', 'close', 'low', 'high']])
      z_company_wise[i]['volume'] = z_volume_scaler.fit_transform(z_company_wise[i][['volume']])


  def generate_dataset(self, company, other_companies_to_include = [], input_rows = ['close'], window_size=20):
    # first column name in input_rows should be the stock which we want to predict, generally close
    df1 = z_company_wise[company][input_rows]

    if len(other_companies_to_include) > 0:
      for comp in other_companies_to_include:
        new_df = z_company_wise[comp][input_rows]
        df1 = df1.merge(new_df, how='inner', on='datetime')

    df = df1.copy()

    df.loc[:, 'day'] = df.index.day
    df.loc[:, 'week'] = df.index.isocalendar().week
    df.loc[:, 'min'] = df.index.minute
    df.loc[:, 'hour'] = df.index.hour
    df['day'] = df['day'] / 31
    df['week'] = df['week'] / 52
    df['min'] = df['min'] / 60
    df['hour'] = df['hour'] / 24

    df_np = df.to_numpy().astype(float)
    data = []

    for index in range(len(df_np) - window_size):
        data.append(df_np[index : index + window_size])

    data = np.array(data)

    x_value = data[:,:-1,:]
    y_value = data[:,-1,0].reshape(-1, 1)

    self.x_value = torch.from_numpy(x_value).type(torch.Tensor)
    self.y_value = torch.from_numpy(y_value).type(torch.Tensor)
    print("Dataloader Created Successfully")
    print("X : value_shape:",x_value.shape)
    print("Y : value_shape", y_value.shape)

"""### Predicting Close Price for Single Company (AAPL) only using its Multiple columns as input and also added new Date/Time columns
- We will train the data on the data for a quarter (4 months) and predict it for the next quarter
"""

dt_dataloader = LSTMTrainDataLoaderWithTime(['AAPL'], '2020-01-01 00:00:00', '2020-04-30 23:59:59')

dt_dataloader.generate_dataset('AAPL', [], ['close', 'open', 'high', 'low', 'volume'], batch_size=batch_size, shuffle=True, window_size=25)

input_dim = 9
hidden_dim = 32
num_layers = 2
output_dim = 1
num_epochs = 100
batch_size = 64

dt_model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)
criterion = torch.nn.MSELoss(reduction='mean')
optimiser = torch.optim.Adagrad(dt_model.parameters(), lr=0.005)
dt_model

hist = train_model(dt_model, num_epochs, dt_dataloader, 10)

torch.save(dt_model, 'multiple_feature_with_datetime.pt')

visualize_train_results(dt_model, dt_dataloader, "Apple Stock Price", "Closing Price")

visualize_test_results(dt_model, dt_dataloader, "Apple Stock Price", "Closing Price")

# get data for next quarter (next four month)
dt_predict_dataloader = LSTMPredictDataLoaderWithTime(['AAPL'], '2020-05-01 00:00:00', '2020-09-01 23:59:59')
dt_predict_dataloader.generate_dataset('AAPL', [], input_rows=['close', 'open', 'high', 'low', 'volume'], window_size=25)

times = [100, 200, 500, 24*60, 7*24*60, 14*24*60, 30*24*60, 58*24*60]
headers = ['100 minutes', '200 minutes', '500 minutes', '1 day', '1 week', '2 weeks', '1 month', '2 months']
dt_mape_vec = []
for j, i in enumerate(times):
  dt_mape_vec.append(float(predict_for_next_timeframe(dt_predict_dataloader, dt_model, i, f"Predicted Apple Stock Price {headers[j]} into the future", "Closing Price")))

"""### Comparisons with other methods done so far"""

def visualize_mape(headers, *mape_vecs):
    x = np.array(range(len(headers)))
    plt.figure(figsize=(14, 5))
    plt.bar(x, mape_vecs[0], label='single input (close)', color='blue', alpha=0.7, width=0.1)
    plt.bar(x + 0.1, mape_vecs[1], label='multiple input (close, open, high, low, volume)', color='red', alpha=0.7, width=0.1)
    plt.bar(x + 0.2, mape_vecs[2], label='multiple stock (AMD + AAPL), multiple input (close, open, high, low, volume)', color='green', alpha=0.7, width=0.1)
    plt.bar(x + 0.3, mape_vecs[3], label='multiple input (close, open, high, low, volume), with datetime data', color='orange', alpha=0.7, width=0.1)

    # Adding labels and title
    plt.xticks(x, headers)
    plt.xlabel('Minutes into the Future')
    plt.ylabel('Price Prediction Error (MAPE)')
    plt.title('Comparison of various methods')
    plt.legend()

    # Adjusting the layout for better visibility
    plt.tight_layout()

    # Show the plot
    plt.show()

visualize_mape(headers, ms_mape_vec, single_mape_vec, multiple_mape_vec, dt_mape_vec)

"""## 9b) Inferences
- Thus we see that adding the time data improves the predictions for shorter timeframe since last trained, that is upto 2 weeks (where it even performs better than single input model). However the model deteriorates even worse than multiple input after considerable about of time not training (1-2 months)
- It performs poorly compared to the models with "multiple input single stock" and "multiple input multiple stock"

### The conclusion is that amongst the various models we implemented in this assignment, the most robust and versatile one is "multiple input, single stock" followed by "multiple input, multiple stock".
"""